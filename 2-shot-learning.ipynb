{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a314bb4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-10-05T07:53:08.256289Z",
     "iopub.status.busy": "2021-10-05T07:53:08.255161Z",
     "iopub.status.idle": "2021-10-05T07:53:12.937891Z",
     "shell.execute_reply": "2021-10-05T07:53:12.936711Z",
     "shell.execute_reply.started": "2021-10-05T06:00:02.085810Z"
    },
    "papermill": {
     "duration": 4.838674,
     "end_time": "2021-10-05T07:53:12.938074",
     "exception": false,
     "start_time": "2021-10-05T07:53:08.099400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Bernoulli\n",
    "from torch.utils.data import Sampler\n",
    "import torchvision.datasets as datasets\n",
    "from functools import partial\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93efab4",
   "metadata": {
    "papermill": {
     "duration": 0.011292,
     "end_time": "2021-10-05T07:53:12.963351",
     "exception": false,
     "start_time": "2021-10-05T07:53:12.952059",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Path Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d158a2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T07:53:12.993388Z",
     "iopub.status.busy": "2021-10-05T07:53:12.992168Z",
     "iopub.status.idle": "2021-10-05T07:53:12.994939Z",
     "shell.execute_reply": "2021-10-05T07:53:12.994333Z",
     "shell.execute_reply.started": "2021-10-05T06:00:06.731165Z"
    },
    "papermill": {
     "duration": 0.020179,
     "end_time": "2021-10-05T07:53:12.995088",
     "exception": false,
     "start_time": "2021-10-05T07:53:12.974909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Path_Manager(fewshot_path):\n",
    "    train = os.path.join(fewshot_path,'train')\n",
    "    val = os.path.join(fewshot_path,'valid')\n",
    "    return train,val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9966fa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T07:53:13.039142Z",
     "iopub.status.busy": "2021-10-05T07:53:13.038136Z",
     "iopub.status.idle": "2021-10-05T07:53:13.040642Z",
     "shell.execute_reply": "2021-10-05T07:53:13.041153Z",
     "shell.execute_reply.started": "2021-10-05T06:00:07.855235Z"
    },
    "papermill": {
     "duration": 0.034684,
     "end_time": "2021-10-05T07:53:13.041327",
     "exception": false,
     "start_time": "2021-10-05T07:53:13.006643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_transform(is_training=None):\n",
    "\n",
    "    mean=[0.485,0.456,0.406]\n",
    "    std=[0.229,0.224,0.225]\n",
    "\n",
    "    normalize = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=mean,std=std)\n",
    "                                    ])\n",
    "\n",
    "    if is_training:\n",
    "        size_transform = transforms.Resize([84,84])\n",
    "        train_transform = transforms.Compose([size_transform,\n",
    "                                            transforms.ColorJitter(brightness=0.4,contrast=0.4,saturation=0.4),\n",
    "                                            transforms.RandomHorizontalFlip(),\n",
    "                                            normalize\n",
    "                                            ])\n",
    "        return train_transform\n",
    "    \n",
    "    else:  \n",
    "        size_transform = transforms.Resize([84,84])\n",
    "        eval_transform = transforms.Compose([size_transform,normalize])\n",
    "        \n",
    "        return eval_transform\n",
    "    \n",
    "    \n",
    "def image_loader(path,is_training):\n",
    "\n",
    "    p = Image.open(path)\n",
    "    p = p.convert('RGB')\n",
    "\n",
    "    final_transform = get_transform(is_training=is_training)\n",
    "\n",
    "    p = final_transform(p)\n",
    "\n",
    "    return p\n",
    "\n",
    "def get_dataset(data_path,is_training):\n",
    "\n",
    "    dataset = datasets.ImageFolder(\n",
    "        data_path,\n",
    "        loader = lambda x: image_loader(path=x,is_training=is_training))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# sampler used for meta-training\n",
    "class meta_batchsampler(Sampler):\n",
    "    \n",
    "    def __init__(self,data_source,way,shots):\n",
    "\n",
    "        self.way = way\n",
    "        self.shots = shots\n",
    "\n",
    "        class2id = {}\n",
    "\n",
    "        for i,(image_path,class_id) in enumerate(data_source.imgs):\n",
    "            if class_id not in class2id:\n",
    "                class2id[class_id]=[]\n",
    "            class2id[class_id].append(i)\n",
    "\n",
    "        self.class2id = class2id\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        temp_class2id = deepcopy(self.class2id)\n",
    "        for class_id in temp_class2id:\n",
    "            np.random.shuffle(temp_class2id[class_id])       \n",
    "\n",
    "        while len(temp_class2id) >= self.way:\n",
    "\n",
    "            id_list = []\n",
    "\n",
    "            list_class_id = list(temp_class2id.keys())\n",
    "\n",
    "            pcount = np.array([len(temp_class2id[class_id]) for class_id in list_class_id])\n",
    "\n",
    "            batch_class_id = np.random.choice(list_class_id,size=self.way,replace=False,p=pcount/sum(pcount))\n",
    "\n",
    "            for shot in self.shots:\n",
    "                for class_id in batch_class_id:\n",
    "                    for _ in range(shot):\n",
    "                        id_list.append(temp_class2id[class_id].pop())\n",
    "\n",
    "            for class_id in batch_class_id:\n",
    "                if len(temp_class2id[class_id])<sum(self.shots):\n",
    "                    temp_class2id.pop(class_id)\n",
    "\n",
    "            yield id_list\n",
    "            \n",
    "def meta_train_dataloader(data_path,way,shots):\n",
    "\n",
    "    dataset = get_dataset(data_path=data_path,is_training=True)\n",
    "#     print(dataset)\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_sampler = meta_batchsampler(data_source=dataset,way=way,shots=shots),\n",
    "        num_workers = 3,\n",
    "        pin_memory = False)\n",
    "\n",
    "    return loader\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c0f8ed",
   "metadata": {
    "papermill": {
     "duration": 0.01115,
     "end_time": "2021-10-05T07:53:13.064217",
     "exception": false,
     "start_time": "2021-10-05T07:53:13.053067",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62b30fb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T07:53:13.133350Z",
     "iopub.status.busy": "2021-10-05T07:53:13.101288Z",
     "iopub.status.idle": "2021-10-05T07:53:13.137642Z",
     "shell.execute_reply": "2021-10-05T07:53:13.136995Z",
     "shell.execute_reply.started": "2021-10-05T06:00:08.749060Z"
    },
    "papermill": {
     "duration": 0.062011,
     "end_time": "2021-10-05T07:53:13.137796",
     "exception": false,
     "start_time": "2021-10-05T07:53:13.075785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class DropBlock(nn.Module):\n",
    "    def __init__(self, block_size):\n",
    "        super(DropBlock, self).__init__()\n",
    "\n",
    "        self.block_size = block_size\n",
    "\n",
    "\n",
    "    def forward(self, x, gamma):\n",
    "        # shape: (bsize, channels, height, width)\n",
    "\n",
    "        if self.training:\n",
    "            batch_size, channels, height, width = x.shape\n",
    "            \n",
    "            bernoulli = Bernoulli(gamma)\n",
    "            mask = bernoulli.sample((batch_size, channels, height - (self.block_size - 1), width - (self.block_size - 1))).cuda()\n",
    "            block_mask = self._compute_block_mask(mask)\n",
    "            countM = block_mask.size()[0] * block_mask.size()[1] * block_mask.size()[2] * block_mask.size()[3]\n",
    "            count_ones = block_mask.sum()\n",
    "\n",
    "            return block_mask * x * (countM / count_ones)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def _compute_block_mask(self, mask):\n",
    "        left_padding = int((self.block_size-1) / 2)\n",
    "        right_padding = int(self.block_size / 2)\n",
    "        \n",
    "        batch_size, channels, height, width = mask.shape\n",
    "        non_zero_idxs = mask.nonzero()\n",
    "        nr_blocks = non_zero_idxs.shape[0]\n",
    "\n",
    "        offsets = torch.stack(\n",
    "            [\n",
    "                torch.arange(self.block_size).view(-1, 1).expand(self.block_size, self.block_size).reshape(-1), # - left_padding,\n",
    "                torch.arange(self.block_size).repeat(self.block_size), #- left_padding\n",
    "            ]\n",
    "        ).t().cuda()\n",
    "        offsets = torch.cat((torch.zeros(self.block_size**2, 2).cuda().long(), offsets.long()), 1)\n",
    "        \n",
    "        if nr_blocks > 0:\n",
    "            non_zero_idxs = non_zero_idxs.repeat(self.block_size ** 2, 1)\n",
    "            offsets = offsets.repeat(nr_blocks, 1).view(-1, 4)\n",
    "            offsets = offsets.long()\n",
    "\n",
    "            block_idxs = non_zero_idxs + offsets\n",
    "            padded_mask = F.pad(mask, (left_padding, right_padding, left_padding, right_padding))\n",
    "            padded_mask[block_idxs[:, 0], block_idxs[:, 1], block_idxs[:, 2], block_idxs[:, 3]] = 1.\n",
    "        else:\n",
    "            padded_mask = F.pad(mask, (left_padding, right_padding, left_padding, right_padding))\n",
    "            \n",
    "        block_mask = 1 - padded_mask#[:height, :width]\n",
    "        return block_mask\n",
    "    \n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, drop_rate=0.0, drop_block=False,\n",
    "                 block_size=1,max_pool=True):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.LeakyReLU(0.1)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = conv3x3(planes, planes)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.maxpool = nn.MaxPool2d(stride)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.drop_rate = drop_rate\n",
    "        self.num_batches_tracked = 0\n",
    "        self.drop_block = drop_block\n",
    "        self.block_size = block_size\n",
    "        self.DropBlock = DropBlock(block_size=self.block_size)\n",
    "        self.max_pool = max_pool\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.num_batches_tracked += 1\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        if self.max_pool:\n",
    "            out = self.maxpool(out)\n",
    "\n",
    "        if self.drop_rate > 0:\n",
    "            if self.drop_block == True:\n",
    "                feat_size = out.size()[2]\n",
    "                keep_rate = max(1.0 - self.drop_rate / (20*2000) * (self.num_batches_tracked), 1.0 - self.drop_rate)\n",
    "                gamma = (1 - keep_rate) / self.block_size**2 * feat_size**2 / (feat_size - self.block_size + 1)**2\n",
    "                out = self.DropBlock(out, gamma=gamma)\n",
    "            else:\n",
    "                out = F.dropout(out, p=self.drop_rate, training=self.training, inplace=True)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, n_blocks, drop_rate=0.0, dropblock_size=5, max_pool=True):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.inplanes = 3\n",
    "        self.layer1 = self._make_layer(block, n_blocks[0], 64,\n",
    "                                       stride=2, drop_rate=drop_rate)\n",
    "        self.layer2 = self._make_layer(block, n_blocks[1], 160,\n",
    "                                       stride=2, drop_rate=drop_rate)\n",
    "        self.layer3 = self._make_layer(block, n_blocks[2], 320,\n",
    "                                       stride=2, drop_rate=drop_rate, drop_block=True, block_size=dropblock_size)\n",
    "        self.layer4 = self._make_layer(block, n_blocks[3], 640,\n",
    "                                       stride=2, drop_rate=drop_rate, drop_block=True, block_size=dropblock_size,max_pool=max_pool)\n",
    "\n",
    "        self.drop_rate = drop_rate\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "    def _make_layer(self, block, n_block, planes, stride=1, drop_rate=0.0, drop_block=False, block_size=1,max_pool=True):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=1, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        if n_block == 1:\n",
    "            layer = block(self.inplanes, planes, stride, downsample, drop_rate, drop_block, block_size,max_pool=max_pool)\n",
    "        else:\n",
    "            layer = block(self.inplanes, planes, stride, downsample, drop_rate)\n",
    "        layers.append(layer)\n",
    "        self.inplanes = planes * block.expansion\n",
    "\n",
    "        for i in range(1, n_block):\n",
    "            if i == n_block - 1:\n",
    "                layer = block(self.inplanes, planes, drop_rate=drop_rate, drop_block=drop_block,\n",
    "                              block_size=block_size)\n",
    "            else:\n",
    "                layer = block(self.inplanes, planes, drop_rate=drop_rate)\n",
    "            layers.append(layer)\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, is_feat=False):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def resnet12(drop_rate=0.0, max_pool=True, **kwargs):\n",
    "        \"\"\"Constructs a ResNet-12 model.\n",
    "        \"\"\"\n",
    "        model = ResNet(BasicBlock, [1, 1, 1, 1], drop_rate=drop_rate, max_pool=max_pool, **kwargs)\n",
    "        return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba1209f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T07:53:13.188190Z",
     "iopub.status.busy": "2021-10-05T07:53:13.186212Z",
     "iopub.status.idle": "2021-10-05T07:53:13.188908Z",
     "shell.execute_reply": "2021-10-05T07:53:13.189372Z",
     "shell.execute_reply.started": "2021-10-05T06:00:09.554892Z"
    },
    "papermill": {
     "duration": 0.038982,
     "end_time": "2021-10-05T07:53:13.189543",
     "exception": false,
     "start_time": "2021-10-05T07:53:13.150561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FRN(nn.Module):\n",
    "    \n",
    "    def __init__(self,way=None,shots=None,is_pretraining=False,num_cat=None):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        num_channel = 640\n",
    "        self.feature_extractor = ResNet.resnet12()\n",
    "\n",
    "        self.shots = shots\n",
    "        self.way = way\n",
    "\n",
    "        # number of channels for the feature map, correspond to d in the paper\n",
    "        self.d = num_channel\n",
    "        \n",
    "        # temperature scaling, correspond to gamma in the paper\n",
    "        self.scale = nn.Parameter(torch.FloatTensor([1.0]),requires_grad=True)\n",
    "        \n",
    "        # H*W=5*5=25, resolution of feature map, correspond to r in the paper\n",
    "        self.resolution = 25\n",
    "\n",
    "        # correpond to [alpha, beta] in the paper\n",
    "        # if is during pre-training, we fix them to 0\n",
    "        self.r = nn.Parameter(torch.zeros(2),requires_grad=not is_pretraining)  \n",
    "    \n",
    "\n",
    "    def get_feature_map(self,inp):\n",
    "\n",
    "        batch_size = inp.size(0)\n",
    "        feature_map = self.feature_extractor(inp)\n",
    "        \n",
    "        feature_map = feature_map/np.sqrt(640)\n",
    "        \n",
    "        return feature_map.view(batch_size,self.d,-1).permute(0,2,1).contiguous() # N,HW,C\n",
    "    \n",
    "\n",
    "    def get_recon_dist(self,query,support,alpha,beta,Woodbury=True):\n",
    "    # query: way*query_shot*resolution, d\n",
    "    # support: way, shot*resolution , d\n",
    "    # Woodbury: whether to use the Woodbury Identity as the implementation or not\n",
    "\n",
    "        # correspond to kr/d in the paper\n",
    "        reg = support.size(1)/support.size(2)\n",
    "        \n",
    "        # correspond to lambda in the paper\n",
    "        lam = reg*alpha.exp()+1e-6\n",
    "\n",
    "        # correspond to gamma in the paper\n",
    "        rho = beta.exp()\n",
    "\n",
    "        st = support.permute(0,2,1) # way, d, shot*resolution\n",
    "\n",
    "        if Woodbury:\n",
    "            # correspond to Equation 10 in the paper\n",
    "            \n",
    "            sts = st.matmul(support) # way, d, d\n",
    "            m_inv = (sts+torch.eye(sts.size(-1)).to(sts.device).unsqueeze(0).mul(lam)).inverse() # way, d, d\n",
    "            hat = m_inv.matmul(sts) # way, d, d\n",
    "        \n",
    "        else:\n",
    "            # correspond to Equation 8 in the paper\n",
    "            \n",
    "            sst = support.matmul(st) # way, shot*resolution, shot*resolution\n",
    "            m_inv = (sst+torch.eye(sst.size(-1)).to(sst.device).unsqueeze(0).mul(lam)).inverse() # way, shot*resolution, shot*resolutionsf \n",
    "            hat = st.matmul(m_inv).matmul(support) # way, d, d\n",
    "\n",
    "        Q_bar = query.matmul(hat).mul(rho) # way, way*query_shot*resolution, d\n",
    "\n",
    "        dist = (Q_bar-query.unsqueeze(0)).pow(2).sum(2).permute(1,0) # way*query_shot*resolution, way\n",
    "        \n",
    "        return dist\n",
    "\n",
    "    \n",
    "    def get_neg_l2_dist(self,inp,way,shot,query_shot,return_support=False):\n",
    "        \n",
    "        resolution = self.resolution\n",
    "        d = self.d\n",
    "        alpha = self.r[0]\n",
    "        beta = self.r[1]\n",
    "        \n",
    "        feature_map = self.get_feature_map(inp)\n",
    "        support = feature_map[:way*shot].view(way, shot*resolution , d)\n",
    "        query = feature_map[way*shot:].view(way*query_shot*resolution, d)\n",
    "        recon_dist = self.get_recon_dist(query=query,support=support,alpha=alpha,beta=beta) # way*query_shot*resolution, way\n",
    "        neg_l2_dist = recon_dist.neg().view(way*query_shot,resolution,way).mean(1) # way*query_shot, way\n",
    "        if return_support:\n",
    "            return neg_l2_dist, support\n",
    "        else:\n",
    "            return neg_l2_dist\n",
    "\n",
    "\n",
    "    def meta_test(self,inp,way,shot,query_shot):\n",
    "        neg_l2_dist = self.get_neg_l2_dist(inp=inp,\n",
    "                                        way=way,\n",
    "                                        shot=shot,\n",
    "                                        query_shot=query_shot)\n",
    "\n",
    "        _,max_index = torch.max(neg_l2_dist,1)\n",
    "\n",
    "        return max_index\n",
    "\n",
    "\n",
    "    def forward(self,inp):\n",
    "\n",
    "        neg_l2_dist, support = self.get_neg_l2_dist(inp=inp,\n",
    "                                                    way=self.way,\n",
    "                                                    shot=self.shots[0],\n",
    "                                                    query_shot=self.shots[1],\n",
    "                                                    return_support=True)\n",
    "            \n",
    "        logits = neg_l2_dist*self.scale\n",
    "        log_prediction = F.log_softmax(logits,dim=1)\n",
    "\n",
    "        return log_prediction, support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4366efb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T07:53:13.227846Z",
     "iopub.status.busy": "2021-10-05T07:53:13.226790Z",
     "iopub.status.idle": "2021-10-05T07:53:13.233430Z",
     "shell.execute_reply": "2021-10-05T07:53:13.232925Z",
     "shell.execute_reply.started": "2021-10-05T06:00:10.199979Z"
    },
    "papermill": {
     "duration": 0.032653,
     "end_time": "2021-10-05T07:53:13.233607",
     "exception": false,
     "start_time": "2021-10-05T07:53:13.200954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def auxrank(support):\n",
    "        way = support.size(0)\n",
    "        shot = support.size(1)\n",
    "        support = support/support.norm(2).unsqueeze(-1)\n",
    "        L1 = torch.zeros((way**2-way)//2).long().cuda()\n",
    "        L2 = torch.zeros((way**2-way)//2).long().cuda()\n",
    "        counter = 0\n",
    "        for i in range(way):\n",
    "            for j in range(i):\n",
    "                L1[counter] = i\n",
    "                L2[counter] = j\n",
    "                counter += 1\n",
    "        s1 = support.index_select(0, L1) # (s^2-s)/2, s, d\n",
    "        s2 = support.index_select(0, L2) # (s^2-s)/2, s, d\n",
    "        dists = s1.matmul(s2.permute(0,2,1)) # (s^2-s)/2, s, s\n",
    "        assert dists.size(-1)==shot\n",
    "        frobs = dists.pow(2).sum(-1).sum(-1)\n",
    "        return frobs.sum().mul(.03)\n",
    "    \n",
    "class FRN_Train:\n",
    "    \n",
    "    def default_train(train_loader,model,optimizer,iter_counter):\n",
    "\n",
    "        way = model.way\n",
    "        query_shot = model.shots[-1]\n",
    "        target = torch.LongTensor([i//query_shot for i in range(query_shot*way)]).cuda()\n",
    "        criterion = nn.NLLLoss().cuda()\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "\n",
    "        avg_frn_loss = 0\n",
    "        avg_aux_loss = 0\n",
    "        avg_loss = 0\n",
    "        avg_acc = 0\n",
    "\n",
    "        for i, (inp,_) in enumerate(train_loader):\n",
    "            \n",
    "            iter_counter += 1\n",
    "            inp = inp.cuda()\n",
    "            log_prediction, s = model(inp)\n",
    "            frn_loss = criterion(log_prediction,target)\n",
    "            aux_loss = auxrank(s)\n",
    "            loss = frn_loss + aux_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _,max_index = torch.max(log_prediction,1)\n",
    "            acc = 100*torch.sum(torch.eq(max_index,target)).item()/query_shot/way\n",
    "            avg_acc += acc\n",
    "            avg_frn_loss += frn_loss.item()\n",
    "            avg_aux_loss += aux_loss.item()\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "        avg_acc = avg_acc/(i+1)\n",
    "        avg_loss = avg_loss/(i+1)\n",
    "        avg_aux_loss = avg_aux_loss/(i+1)\n",
    "        avg_frn_loss = avg_frn_loss/(i+1)\n",
    "\n",
    "        return iter_counter,avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47878adb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T07:53:13.277251Z",
     "iopub.status.busy": "2021-10-05T07:53:13.275295Z",
     "iopub.status.idle": "2021-10-05T07:53:13.280223Z",
     "shell.execute_reply": "2021-10-05T07:53:13.279726Z",
     "shell.execute_reply.started": "2021-10-05T06:00:10.835669Z"
    },
    "papermill": {
     "duration": 0.035297,
     "end_time": "2021-10-05T07:53:13.280366",
     "exception": false,
     "start_time": "2021-10-05T07:53:13.245069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sampler used for meta-testing\n",
    "class random_sampler(Sampler):\n",
    "\n",
    "    def __init__(self,data_source,way,shot,query_shot=16,trial=1000):\n",
    "\n",
    "        class2id = {}\n",
    "\n",
    "        for i,(image_path,class_id) in enumerate(data_source.imgs):\n",
    "            if class_id not in class2id:\n",
    "                class2id[class_id]=[]\n",
    "            class2id[class_id].append(i)\n",
    "\n",
    "        self.class2id = class2id\n",
    "        self.way = way\n",
    "        self.shot = shot\n",
    "        self.trial = trial\n",
    "        self.query_shot = query_shot\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        way = self.way\n",
    "        shot = self.shot\n",
    "        trial = self.trial\n",
    "        query_shot = self.query_shot\n",
    "        \n",
    "        class2id = deepcopy(self.class2id)        \n",
    "        list_class_id = list(class2id.keys())\n",
    "\n",
    "        for i in range(trial):\n",
    "\n",
    "            id_list = []\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(list_class_id)\n",
    "            picked_class = list_class_id[:way]\n",
    "\n",
    "            for cat in picked_class:\n",
    "                np.random.shuffle(class2id[cat])\n",
    "                \n",
    "            for cat in picked_class:\n",
    "                id_list.extend(class2id[cat][:shot])\n",
    "            for cat in picked_class:\n",
    "                id_list.extend(class2id[cat][shot:(shot+query_shot)])\n",
    "\n",
    "            yield id_list\n",
    "            \n",
    "def get_score(acc_list):\n",
    "\n",
    "    mean = np.mean(acc_list)\n",
    "    interval = 1.96*np.sqrt(np.var(acc_list)/len(acc_list))\n",
    "\n",
    "    return mean,interval\n",
    "\n",
    "def meta_test_dataloader(data_path,way,shot,query_shot=16,trial=1000):\n",
    "\n",
    "    dataset = get_dataset(data_path=data_path,is_training=False)\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_sampler = random_sampler(data_source=dataset,way=way,shot=shot,query_shot=query_shot,trial=trial),\n",
    "        num_workers = 3,\n",
    "        pin_memory = False)\n",
    "\n",
    "    return loader\n",
    "\n",
    "def meta_test(data_path,model,way,shot,query_shot=16,trial=10000,return_list=False):\n",
    "\n",
    "    eval_loader = meta_test_dataloader(data_path=data_path,\n",
    "                                                way=way,\n",
    "                                                shot=shot,\n",
    "                                                query_shot=query_shot,\n",
    "                                                trial=trial)\n",
    "    \n",
    "    target = torch.LongTensor([i//query_shot for i in range(query_shot*way)]).cuda()\n",
    "\n",
    "    acc_list = []\n",
    "\n",
    "    for i, (inp,_) in tqdm(enumerate(eval_loader)):\n",
    "\n",
    "        inp = inp.cuda()\n",
    "        max_index = model.meta_test(inp,way=way,shot=shot,query_shot=query_shot)\n",
    "        acc = 100*torch.sum(torch.eq(max_index,target)).item()/query_shot/way\n",
    "        acc_list.append(acc)\n",
    "\n",
    "    if return_list:\n",
    "        return np.array(acc_list)\n",
    "    else:\n",
    "        mean,interval = get_score(acc_list)\n",
    "        return mean,interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fafa26f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T07:53:13.330951Z",
     "iopub.status.busy": "2021-10-05T07:53:13.328985Z",
     "iopub.status.idle": "2021-10-05T07:53:13.331634Z",
     "shell.execute_reply": "2021-10-05T07:53:13.332166Z",
     "shell.execute_reply.started": "2021-10-05T06:04:26.278106Z"
    },
    "papermill": {
     "duration": 0.039981,
     "end_time": "2021-10-05T07:53:13.332370",
     "exception": false,
     "start_time": "2021-10-05T07:53:13.292389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_opt(model, lr, weight_decay,epoch,gamma):\n",
    "\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(),lr= lr,momentum=0.9,weight_decay=weight_decay,nesterov=True)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer,step_size=epoch,gamma=gamma)\n",
    "\n",
    "    return optimizer,scheduler\n",
    "\n",
    "\n",
    "class Train_Manager:\n",
    "\n",
    "    def __init__(self,path_manager,train_func):\n",
    "\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.cuda.set_device(0)\n",
    "\n",
    "        self.name = 'ResNet-12'\n",
    "        self.opt = 'sgd'\n",
    "        self.lr = 1e-1\n",
    "        self.gamma = 1e-1\n",
    "        self.epoch = 100\n",
    "        self.stage = 2\n",
    "        self.weight_decay = 5e-4\n",
    "        self.train_way = 10\n",
    "        \n",
    "        suffix = '%s-lr_%.0e-gamma_%.0e-epoch_%d-stage_%d-decay_%.0e-way_%d' % (self.opt,\n",
    "                self.lr,self.gamma,self.epoch,self.stage,self.weight_decay,self.train_way)\n",
    "\n",
    "        name = \"%s-%s\"%(self.name,suffix)\n",
    "\n",
    "        self.save_path = 'model_%s.pth' % (name)\n",
    "\n",
    "        self.train_func = train_func\n",
    "        self.pm = path_manager\n",
    "\n",
    "    def train(self,model):\n",
    "\n",
    "        \n",
    "        train_func = self.train_func\n",
    "        save_path = self.save_path\n",
    "\n",
    "        optimizer,scheduler = get_opt(model, self.lr, self.weight_decay,self.epoch, self.gamma)\n",
    "\n",
    "        val_shot = 1\n",
    "        test_way = 194\n",
    "\n",
    "        best_val_acc = 0\n",
    "        best_epoch = 0\n",
    "\n",
    "        model.train()\n",
    "        model.cuda()\n",
    "\n",
    "        iter_counter = 0\n",
    "\n",
    "        total_epoch = self.epoch*self.stage\n",
    "\n",
    "        print(\"start training!\")\n",
    "\n",
    "        for e in tqdm(range(total_epoch)):\n",
    "\n",
    "            iter_counter,train_acc = train_func(model=model,\n",
    "                                                optimizer=optimizer,\n",
    "                                                iter_counter=iter_counter)\n",
    "\n",
    "            if (e+1)%10==0:\n",
    "\n",
    "                print(\"\")\n",
    "                print(\"epoch %d/%d, iter %d:\" % (e+1,total_epoch,iter_counter))\n",
    "                (\"train_acc: %.3f\" % (train_acc))\n",
    "\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_acc,val_interval = meta_test(data_path=self.pm[1],\n",
    "                                                            model=model,\n",
    "                                                            way=test_way,\n",
    "                                                            shot=val_shot,\n",
    "                                                            query_shot=1,\n",
    "                                                            trial=10)\n",
    "\n",
    "                print('val_%d-way-%d-shot_acc: %.3f\\t%.3f'%(test_way,val_shot,val_acc,val_interval))\n",
    "\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    best_epoch = e+1\n",
    "#                     if not args.no_val:\n",
    "                    torch.save(model.state_dict(),'best_'+save_path)\n",
    "                    print('BEST!')\n",
    "\n",
    "                model.train()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        print('training finished!')\n",
    "#         if args.no_val:\n",
    "        torch.save(model.state_dict(),save_path)\n",
    "\n",
    "        print('------------------------')\n",
    "        print(('the best epoch is %d/%d') % (best_epoch,total_epoch))\n",
    "        print(('the best %d-way %d-shot val acc is %.3f') % (test_way,val_shot,best_val_acc))\n",
    "\n",
    "\n",
    "    def evaluate(self,model):\n",
    "\n",
    "\n",
    "        print('------------------------')\n",
    "        print('evaluating on test set:')\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            model.load_state_dict(torch.load(self.save_path))\n",
    "            model.eval()\n",
    "\n",
    "            for shot in [1]:\n",
    "\n",
    "                mean,interval = meta_test(data_path=self.pm[1],\n",
    "                                        model=model,\n",
    "                                        way=194,\n",
    "                                        shot=1,\n",
    "                                        query_shot=1,\n",
    "                                        trial=10)\n",
    "\n",
    "                print('%d-way-%d-shot acc: %.2f\\t%.2f'%(194,shot,mean,interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2533629f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T07:53:13.365399Z",
     "iopub.status.busy": "2021-10-05T07:53:13.364703Z",
     "iopub.status.idle": "2021-10-05T07:53:14.356910Z",
     "shell.execute_reply": "2021-10-05T07:53:14.356282Z",
     "shell.execute_reply.started": "2021-10-05T06:05:38.601409Z"
    },
    "papermill": {
     "duration": 1.011423,
     "end_time": "2021-10-05T07:53:14.357072",
     "exception": false,
     "start_time": "2021-10-05T07:53:13.345649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = os.path.abspath('../input/snap-retail-data/')\n",
    "fewshot_path = os.path.join(data_path,'final_data')\n",
    "\n",
    "pm = Path_Manager(fewshot_path=fewshot_path)\n",
    "train_way = 10\n",
    "shots = [2, 1]\n",
    "train_loader = meta_train_dataloader(data_path=pm[0],\n",
    "                                                way=train_way,\n",
    "                                                shots=shots)\n",
    "model = FRN(way=train_way,\n",
    "            shots=[2,1])\n",
    "train_func = partial(FRN_Train.default_train,train_loader=train_loader)\n",
    "tm = Train_Manager(path_manager=pm,train_func=train_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a797b3a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-05T07:53:14.388303Z",
     "iopub.status.busy": "2021-10-05T07:53:14.384806Z",
     "iopub.status.idle": "2021-10-05T08:07:37.182039Z",
     "shell.execute_reply": "2021-10-05T08:07:37.181376Z",
     "shell.execute_reply.started": "2021-10-05T06:05:40.142674Z"
    },
    "papermill": {
     "duration": 862.812985,
     "end_time": "2021-10-05T08:07:37.182185",
     "exception": false,
     "start_time": "2021-10-05T07:53:14.369200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 9/200 [00:29<10:05,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 10/200, iter 190:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.75s/it]\u001b[A\n",
      "2it [00:04,  2.06s/it]\u001b[A\n",
      "3it [00:05,  1.56s/it]\u001b[A\n",
      "4it [00:07,  1.57s/it]\u001b[A\n",
      "5it [00:08,  1.31s/it]\u001b[A\n",
      "6it [00:08,  1.15s/it]\u001b[A\n",
      "7it [00:10,  1.16s/it]\u001b[A\n",
      "8it [00:10,  1.05s/it]\u001b[A\n",
      "9it [00:11,  1.02it/s]\u001b[A\n",
      "10it [00:12,  1.26s/it]\n",
      "  5%|▌         | 10/200 [00:45<23:08,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_194-way-1-shot_acc: 55.309\t0.870\n",
      "BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 19/200 [01:13<09:42,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 20/200, iter 380:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.32s/it]\u001b[A\n",
      "2it [00:04,  1.86s/it]\u001b[A\n",
      "3it [00:05,  1.40s/it]\u001b[A\n",
      "4it [00:05,  1.18s/it]\u001b[A\n",
      "5it [00:06,  1.06s/it]\u001b[A\n",
      "6it [00:07,  1.02it/s]\u001b[A\n",
      "7it [00:08,  1.07it/s]\u001b[A\n",
      "8it [00:09,  1.12it/s]\u001b[A\n",
      "9it [00:09,  1.15it/s]\u001b[A\n",
      "10it [00:10,  1.09s/it]\n",
      " 10%|█         | 20/200 [01:27<19:36,  6.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_194-way-1-shot_acc: 56.804\t0.977\n",
      "BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 29/200 [01:55<09:06,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 30/200, iter 570:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.66s/it]\u001b[A\n",
      "2it [00:03,  1.58s/it]\u001b[A\n",
      "3it [00:04,  1.26s/it]\u001b[A\n",
      "4it [00:05,  1.16s/it]\u001b[A\n",
      "5it [00:06,  1.05s/it]\u001b[A\n",
      "6it [00:07,  1.03it/s]\u001b[A\n",
      "7it [00:07,  1.08it/s]\u001b[A\n",
      "8it [00:08,  1.12it/s]\u001b[A\n",
      "9it [00:09,  1.15it/s]\u001b[A\n",
      "10it [00:10,  1.04s/it]\n",
      " 15%|█▌        | 30/200 [02:09<18:00,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_194-way-1-shot_acc: 57.887\t1.238\n",
      "BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 39/200 [02:36<08:33,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 40/200, iter 760:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.67s/it]\u001b[A\n",
      "2it [00:03,  1.59s/it]\u001b[A\n",
      "3it [00:04,  1.24s/it]\u001b[A\n",
      "4it [00:05,  1.13s/it]\u001b[A\n",
      "5it [00:06,  1.02s/it]\u001b[A\n",
      "6it [00:06,  1.05it/s]\u001b[A\n",
      "7it [00:08,  1.09s/it]\u001b[A\n",
      "8it [00:09,  1.01s/it]\u001b[A\n",
      "9it [00:09,  1.05it/s]\u001b[A\n",
      "10it [00:10,  1.09s/it]\n",
      " 20%|██        | 40/200 [02:50<17:22,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_194-way-1-shot_acc: 60.361\t1.336\n",
      "BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 49/200 [03:18<08:01,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 50/200, iter 950:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.16s/it]\u001b[A\n",
      "2it [00:04,  1.80s/it]\u001b[A\n",
      "3it [00:04,  1.37s/it]\u001b[A\n",
      "4it [00:05,  1.23s/it]\u001b[A\n",
      "5it [00:06,  1.10s/it]\u001b[A\n",
      "6it [00:07,  1.01s/it]\u001b[A\n",
      "7it [00:08,  1.05s/it]\u001b[A\n",
      "8it [00:09,  1.02it/s]\u001b[A\n",
      "9it [00:10,  1.07it/s]\u001b[A\n",
      "10it [00:11,  1.12s/it]\n",
      " 25%|██▌       | 50/200 [03:33<16:46,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_194-way-1-shot_acc: 65.000\t1.632\n",
      "BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 59/200 [04:01<07:44,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 60/200, iter 1140:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.91s/it]\u001b[A\n",
      "2it [00:03,  1.71s/it]\u001b[A\n",
      "3it [00:04,  1.32s/it]\u001b[A\n",
      "4it [00:05,  1.32s/it]\u001b[A\n",
      "5it [00:06,  1.15s/it]\u001b[A\n",
      "6it [00:07,  1.04s/it]\u001b[A\n",
      "7it [00:08,  1.05s/it]\u001b[A\n",
      "8it [00:09,  1.02it/s]\u001b[A\n",
      "9it [00:10,  1.07it/s]\u001b[A\n",
      "10it [00:11,  1.12s/it]\n",
      " 30%|███       | 60/200 [04:16<15:44,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_194-way-1-shot_acc: 62.680\t1.327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 69/200 [04:45<07:20,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 70/200, iter 1330:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.90s/it]\u001b[A\n",
      "2it [00:03,  1.69s/it]\u001b[A\n",
      "3it [00:04,  1.32s/it]\u001b[A\n",
      "4it [00:05,  1.14s/it]\u001b[A\n",
      "5it [00:06,  1.04s/it]\u001b[A\n",
      "6it [00:07,  1.03it/s]\u001b[A\n",
      "7it [00:08,  1.00it/s]\u001b[A\n",
      "8it [00:09,  1.06it/s]\u001b[A\n",
      "9it [00:09,  1.10it/s]\u001b[A\n",
      "10it [00:10,  1.08s/it]\n",
      " 35%|███▌      | 70/200 [04:59<14:22,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_194-way-1-shot_acc: 61.186\t0.686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 79/200 [05:29<06:45,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 80/200, iter 1520:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.74s/it]\u001b[A\n",
      "2it [00:04,  2.03s/it]\u001b[A\n",
      "3it [00:05,  1.48s/it]\u001b[A\n",
      "4it [00:06,  1.33s/it]\u001b[A\n",
      "5it [00:07,  1.16s/it]\u001b[A\n",
      "6it [00:08,  1.05s/it]\u001b[A\n",
      "7it [00:09,  1.04s/it]\u001b[A\n",
      "8it [00:10,  1.03it/s]\u001b[A\n",
      "9it [00:10,  1.08it/s]\u001b[A\n",
      "10it [00:11,  1.18s/it]\n",
      " 40%|████      | 80/200 [05:44<13:49,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_194-way-1-shot_acc: 64.278\t1.246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 89/200 [06:13<06:12,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 90/200, iter 1710:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.07s/it]\u001b[A\n",
      "2it [00:03,  1.76s/it]\u001b[A\n",
      "3it [00:04,  1.34s/it]\u001b[A\n",
      "4it [00:05,  1.26s/it]\u001b[A\n",
      "5it [00:06,  1.12s/it]\u001b[A\n",
      "6it [00:07,  1.02s/it]\u001b[A\n",
      "7it [00:08,  1.01it/s]\u001b[A\n",
      "8it [00:09,  1.07it/s]\u001b[A\n",
      "9it [00:10,  1.10it/s]\u001b[A\n",
      "10it [00:11,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_194-way-1-shot_acc: 66.753\t1.342\n",
      "BEST!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 90/200 [06:28<12:28,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 99/200 [06:55<05:24,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 100/200, iter 1900:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.83s/it]\u001b[A\n",
      "2it [00:03,  1.66s/it]\u001b[A\n",
      "3it [00:04,  1.30s/it]\u001b[A\n",
      "4it [00:05,  1.13s/it]\u001b[A\n",
      "5it [00:06,  1.03s/it]\u001b[A\n",
      "6it [00:07,  1.03it/s]\u001b[A\n",
      "7it [00:08,  1.03s/it]\u001b[A\n",
      "8it [00:09,  1.04it/s]\u001b[A\n",
      "9it [00:09,  1.09it/s]\u001b[A\n",
      "10it [00:10,  1.08s/it]\n",
      " 50%|█████     | 100/200 [07:10<10:49,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_194-way-1-shot_acc: 65.000\t1.638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 109/200 [07:38<04:57,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 110/200, iter 2090:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.34s/it]\u001b[A\n",
      "2it [00:04,  1.86s/it]\u001b[A\n",
      "3it [00:05,  1.43s/it]\u001b[A\n",
      "4it [00:06,  1.47s/it]\u001b[A\n",
      "5it [00:07,  1.25s/it]\u001b[A\n",
      "6it [00:08,  1.11s/it]\u001b[A\n",
      "7it [00:09,  1.05s/it]\u001b[A\n",
      "8it [00:10,  1.03it/s]\u001b[A\n",
      "9it [00:10,  1.08it/s]\u001b[A\n",
      "10it [00:11,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_194-way-1-shot_acc: 66.856\t1.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 110/200 [07:54<10:19,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 119/200 [08:21<04:27,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 120/200, iter 2280:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.60s/it]\u001b[A\n",
      "2it [00:03,  1.57s/it]\u001b[A\n",
      "3it [00:04,  1.25s/it]\u001b[A\n",
      "4it [00:05,  1.12s/it]\u001b[A\n",
      "5it [00:06,  1.02s/it]\u001b[A\n",
      "6it [00:06,  1.05it/s]\u001b[A\n",
      "7it [00:07,  1.08it/s]\u001b[A\n",
      "8it [00:08,  1.13it/s]\u001b[A\n",
      "9it [00:09,  1.15it/s]\u001b[A\n",
      "10it [00:10,  1.03s/it]\n",
      " 60%|██████    | 120/200 [08:35<08:29,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_194-way-1-shot_acc: 67.062\t0.851\n",
      "BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 129/200 [09:02<03:48,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 130/200, iter 2470:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.69s/it]\u001b[A\n",
      "2it [00:03,  1.60s/it]\u001b[A\n",
      "3it [00:04,  1.25s/it]\u001b[A\n",
      "4it [00:05,  1.09s/it]\u001b[A\n",
      "5it [00:06,  1.01s/it]\u001b[A\n",
      "6it [00:06,  1.06it/s]\u001b[A\n",
      "7it [00:07,  1.10it/s]\u001b[A\n",
      "8it [00:08,  1.14it/s]\u001b[A\n",
      "9it [00:09,  1.17it/s]\u001b[A\n",
      "10it [00:10,  1.02s/it]\n",
      " 65%|██████▌   | 130/200 [09:16<07:21,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_194-way-1-shot_acc: 67.680\t0.715\n",
      "BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 139/200 [09:43<03:10,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 140/200, iter 2660:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.74s/it]\u001b[A\n",
      "2it [00:03,  1.61s/it]\u001b[A\n",
      "3it [00:04,  1.27s/it]\u001b[A\n",
      "4it [00:05,  1.10s/it]\u001b[A\n",
      "5it [00:06,  1.01s/it]\u001b[A\n",
      "6it [00:06,  1.06it/s]\u001b[A\n",
      "7it [00:07,  1.09it/s]\u001b[A\n",
      "8it [00:08,  1.13it/s]\u001b[A\n",
      "9it [00:09,  1.16it/s]\u001b[A\n",
      "10it [00:10,  1.03s/it]\n",
      " 70%|███████   | 140/200 [09:56<06:15,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_194-way-1-shot_acc: 66.804\t0.673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 149/200 [10:24<02:44,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 150/200, iter 2850:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.57s/it]\u001b[A\n",
      "2it [00:04,  1.96s/it]\u001b[A\n",
      "3it [00:05,  1.46s/it]\u001b[A\n",
      "4it [00:06,  1.46s/it]\u001b[A\n",
      "5it [00:07,  1.24s/it]\u001b[A\n",
      "6it [00:08,  1.10s/it]\u001b[A\n",
      "7it [00:09,  1.07s/it]\u001b[A\n",
      "8it [00:10,  1.01it/s]\u001b[A\n",
      "9it [00:11,  1.06it/s]\u001b[A\n",
      "10it [00:11,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_194-way-1-shot_acc: 68.660\t0.586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 150/200 [10:40<05:47,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 159/200 [11:09<02:20,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 160/200, iter 3040:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.95s/it]\u001b[A\n",
      "2it [00:03,  1.71s/it]\u001b[A\n",
      "3it [00:04,  1.32s/it]\u001b[A\n",
      "4it [00:05,  1.13s/it]\u001b[A\n",
      "5it [00:06,  1.03s/it]\u001b[A\n",
      "6it [00:07,  1.04it/s]\u001b[A\n",
      "7it [00:08,  1.04s/it]\u001b[A\n",
      "8it [00:09,  1.03it/s]\u001b[A\n",
      "9it [00:10,  1.08it/s]\u001b[A\n",
      "10it [00:10,  1.09s/it]\n",
      " 80%|████████  | 160/200 [11:23<04:28,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_194-way-1-shot_acc: 68.969\t0.636\n",
      "BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 169/200 [11:52<01:43,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 170/200, iter 3230:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.18s/it]\u001b[A\n",
      "2it [00:04,  1.80s/it]\u001b[A\n",
      "3it [00:04,  1.37s/it]\u001b[A\n",
      "4it [00:06,  1.41s/it]\u001b[A\n",
      "5it [00:07,  1.20s/it]\u001b[A\n",
      "6it [00:07,  1.07s/it]\u001b[A\n",
      "7it [00:09,  1.11s/it]\u001b[A\n",
      "8it [00:09,  1.01s/it]\u001b[A\n",
      "9it [00:10,  1.04it/s]\u001b[A\n",
      "10it [00:11,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_194-way-1-shot_acc: 69.021\t0.524\n",
      "BEST!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 170/200 [12:08<03:29,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 179/200 [12:36<01:09,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 180/200, iter 3420:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.01s/it]\u001b[A\n",
      "2it [00:03,  1.75s/it]\u001b[A\n",
      "3it [00:04,  1.34s/it]\u001b[A\n",
      "4it [00:06,  1.36s/it]\u001b[A\n",
      "5it [00:06,  1.17s/it]\u001b[A\n",
      "6it [00:07,  1.05s/it]\u001b[A\n",
      "7it [00:08,  1.01s/it]\u001b[A\n",
      "8it [00:09,  1.06it/s]\u001b[A\n",
      "9it [00:10,  1.10it/s]\u001b[A\n",
      "10it [00:11,  1.12s/it]\n",
      " 90%|█████████ | 180/200 [12:51<02:14,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_194-way-1-shot_acc: 69.381\t0.703\n",
      "BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 189/200 [13:19<00:36,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 190/200, iter 3610:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.07s/it]\u001b[A\n",
      "2it [00:03,  1.75s/it]\u001b[A\n",
      "3it [00:04,  1.35s/it]\u001b[A\n",
      "4it [00:05,  1.20s/it]\u001b[A\n",
      "5it [00:06,  1.07s/it]\u001b[A\n",
      "6it [00:07,  1.01it/s]\u001b[A\n",
      "7it [00:08,  1.01s/it]\u001b[A\n",
      "8it [00:09,  1.05it/s]\u001b[A\n",
      "9it [00:10,  1.10it/s]\u001b[A\n",
      "10it [00:10,  1.10s/it]\n",
      " 95%|█████████▌| 190/200 [13:33<01:05,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_194-way-1-shot_acc: 69.330\t0.704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 199/200 [14:02<00:03,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 200/200, iter 3800:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.11s/it]\u001b[A\n",
      "2it [00:03,  1.77s/it]\u001b[A\n",
      "3it [00:04,  1.34s/it]\u001b[A\n",
      "4it [00:05,  1.20s/it]\u001b[A\n",
      "5it [00:06,  1.09s/it]\u001b[A\n",
      "6it [00:07,  1.00it/s]\u001b[A\n",
      "7it [00:08,  1.06it/s]\u001b[A\n",
      "8it [00:09,  1.11it/s]\u001b[A\n",
      "9it [00:09,  1.14it/s]\u001b[A\n",
      "10it [00:10,  1.08s/it]\n",
      "100%|██████████| 200/200 [14:16<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_194-way-1-shot_acc: 69.742\t0.607\n",
      "BEST!\n",
      "training finished!\n",
      "------------------------\n",
      "the best epoch is 200/200\n",
      "the best 194-way 1-shot val acc is 69.742\n"
     ]
    }
   ],
   "source": [
    "tm.train(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 878.998657,
   "end_time": "2021-10-05T08:07:39.438272",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-10-05T07:53:00.439615",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
